{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = \"Period1.csv\"\n",
    "file2 = \"Period2.csv\"\n",
    "file3 = \"TestData.csv\"\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data_set = list(reader)\n",
    "        \n",
    "        data_set = [[element[0].split(\" \") for element in data_set],[element[1].split(\" \") for element in data_set],[element[2].split(\" \") for element in data_set]]\n",
    "        del data_set[0][0]\n",
    "        del data_set[1][0]\n",
    "        del data_set[2][0]\n",
    "        return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =readfile(file)\n",
    "data2 = readfile(file2)\n",
    "testdata = readfile(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    features = pd.DataFrame([[data[1][i],data[2][i]]for i in range(len(data[0]))])\n",
    "    features.columns = ['From', 'To']\n",
    "                    \n",
    "    year = [i[0] for i in data] \n",
    "\n",
    "    features['From'] = features['From'].apply(lambda x:x[0])\n",
    "    features['To'] = features['To'].apply(lambda x:x[0])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = create_features(data)\n",
    "features2 = create_features(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph(data):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    linked_nodes = [(data['From'][i],data['To'][i])for i in range(len(data))]\n",
    "    \n",
    "    G.add_edges_from(linked_nodes)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def create_digraph(data):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    linked_nodes = [(data['From'][i],data['To'][i])for i in range(len(data))]\n",
    "    \n",
    "    G.add_edges_from(linked_nodes)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def labeling(features,features2):\n",
    "    label = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        if(a in features2.values or b in features2.values):\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = create_graph(features)\n",
    "graph2 = create_digraph(features)\n",
    "nx.is_directed(graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_neighbors(features, G):\n",
    "    nb_common_neighbors = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        nb_common_neighbors.append(len(sorted(nx.common_neighbors(G, a, b)))) # ajoute le nombre de voisins communs\n",
    "    return nb_common_neighbors\n",
    "\n",
    "def Jaccard_coef(features, G):\n",
    "    J = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        pred = nx.jaccard_coefficient(G,[(a,b)])\n",
    "        for u,v,p in pred:\n",
    "            J.append(p)\n",
    "    return J\n",
    "\n",
    "def betweenness_diff(features, G):\n",
    "    btw = nx.betweenness_centrality(G, 50)\n",
    "    btw_diff = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        btw_diff.append(btw[b] - btw[a])\n",
    "    return btw_diff\n",
    "\n",
    "def in_link_diff(features, G2):\n",
    "    diff = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        diff.append(len(G2.in_edges(b)) - len(G2.in_edges(a)))\n",
    "    return diff\n",
    "\n",
    "def is_same_cluster(partition, features):\n",
    "    same_cluster = []\n",
    "    for i in range(features.shape[0]):\n",
    "        a = features['From'][i]\n",
    "        b = features['To'][i]\n",
    "        if(partition[a] == partition[b]):\n",
    "            same_cluster.append(1)\n",
    "        else:\n",
    "            same_cluster.append(0)\n",
    "    return same_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_common_neighbors = common_neighbors(features, graph)\n",
    "features['No_common_neighbors'] = no_common_neighbors\n",
    "\n",
    "Jaccard = Jaccard_coef(features, graph)\n",
    "features['Jaccard_coef'] = Jaccard\n",
    "\n",
    "btw_diff = betweenness_diff(features, graph)\n",
    "features['Betweenness_diff'] = btw_diff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = in_link_diff(features, graph2)\n",
    "features['In_link_diff'] = diff\n",
    "\n",
    "partition = community.best_partition(graph)\n",
    "same_cluster_train = is_same_cluster(partition, features)\n",
    "features['Is_same_cluster'] = same_cluster_train\n",
    "\n",
    "# result_list.iloc[0:10000].to_csv('result.csv', sep=',')\n",
    "features.to_csv('graph_features_training.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98353\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "graph_test = create_graph(features2)\n",
    "graph_test_2 = create_digraph(features2)\n",
    "\n",
    "no_common_neighbors_test = common_neighbors(features2, graph_test)\n",
    "print(len(no_common_neighbors_test))\n",
    "features2['No_common_neighbors'] = no_common_neighbors_test\n",
    "\n",
    "Jaccard_test = Jaccard_coef(features2, graph_test)\n",
    "features2['Jaccard_coef'] = Jaccard_test\n",
    "\n",
    "btw_diff_test = betweenness_diff(features2, graph_test)\n",
    "features2['Betweenness_diff'] = btw_diff_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "diff2 = in_link_diff(features2, graph_test_2)\n",
    "features2['In_link_diff'] = diff2\n",
    "\n",
    "partition2 = community.best_partition(graph_test)\n",
    "same_cluster_test = is_same_cluster(partition2, features2)\n",
    "features2['Is_same_cluster'] = same_cluster_test\n",
    "\n",
    "# result_list.iloc[0:10000].to_csv('result.csv', sep=',')\n",
    "features2.to_csv('graph_features_testing.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features2['label'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features2.to_csv('graph_features_testing.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = labeling(features, features2)\n",
    "features['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.to_csv('graph_features_training_with_label.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "\n",
    "train = pd.read_csv('graph_features_training.csv')\n",
    "del train['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "X_train = train.as_matrix()\n",
    "\n",
    "y_train = label\n",
    "\n",
    "#testing\n",
    "test = pd.read_csv('graph_features_testing.csv')\n",
    "del test['Unnamed: 0']\n",
    "\n",
    "X_test = test.as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "clf = ExtraTreesClassifier(max_features=None, min_samples_leaf=20, n_estimators = 500, n_jobs=3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra trees classifier\n",
    "\n",
    "clf = ExtraTreesClassifier(max_features=None,min_samples_leaf=10,n_estimators=500,n_jobs=3)\n",
    "cv = cross_validation.cross_val_score(clf, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93311633282\n"
     ]
    }
   ],
   "source": [
    "#xg boost classifier\n",
    "# 1st tuning\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=6, n_estimators=500, learning_rate=0.01)\n",
    "cv = cross_validation.cross_val_score(gbm, X_train, y_train, cv=5)\n",
    "print(np.mean(cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2nd tuning\n",
    "gbm = xgb.XGBClassifier(max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "gbm.fit(X_train, y_train)\n",
    "pred = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV] ... learning_rate=0.05, n_estimators=500, score=0.934511 -  35.6s\n",
      "[CV] learning_rate=0.05, n_estimators=500 ............................\n",
      "[CV] ... learning_rate=0.05, n_estimators=500, score=0.933738 -  35.7s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV] ... learning_rate=0.05, n_estimators=500, score=0.935706 -  35.7s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV] ... learning_rate=0.05, n_estimators=500, score=0.934802 -  35.8s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV] ... learning_rate=0.05, n_estimators=500, score=0.934188 -  35.9s\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. learning_rate=0.05, n_estimators=1000, score=0.935706 - 1.1min\n",
      "[CV] learning_rate=0.05, n_estimators=1000 ...........................\n",
      "[CV] .. learning_rate=0.05, n_estimators=1000, score=0.935318 - 1.2min\n",
      "[CV] learning_rate=0.01, n_estimators=500 ............................\n",
      "[CV] .. learning_rate=0.05, n_estimators=1000, score=0.934126 - 1.2min\n",
      "[CV] learning_rate=0.01, n_estimators=500 ............................\n",
      "[CV] .. learning_rate=0.05, n_estimators=1000, score=0.937030 - 1.1min\n",
      "[CV] learning_rate=0.01, n_estimators=500 ............................\n",
      "[CV] ... learning_rate=0.01, n_estimators=500, score=0.932089 -  36.2s\n",
      "[CV] learning_rate=0.01, n_estimators=500 ............................\n",
      "[CV] ... learning_rate=0.01, n_estimators=500, score=0.931607 -  36.4s\n",
      "[CV] learning_rate=0.01, n_estimators=500 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. learning_rate=0.05, n_estimators=1000, score=0.934220 - 1.1min\n",
      "[CV] learning_rate=0.01, n_estimators=1000 ...........................\n",
      "[CV] ... learning_rate=0.01, n_estimators=500, score=0.932541 -  36.0s\n",
      "[CV] learning_rate=0.01, n_estimators=1000 ...........................\n",
      "[CV] ... learning_rate=0.01, n_estimators=500, score=0.931217 -  35.9s\n",
      "[CV] learning_rate=0.01, n_estimators=1000 ...........................\n",
      "[CV] ... learning_rate=0.01, n_estimators=500, score=0.932380 -  36.2s\n",
      "[CV] learning_rate=0.01, n_estimators=1000 ...........................\n",
      "[CV] .. learning_rate=0.01, n_estimators=1000, score=0.932898 - 1.2min\n",
      "[CV] learning_rate=0.01, n_estimators=1000 ...........................\n",
      "[CV] .. learning_rate=0.01, n_estimators=1000, score=0.932832 - 1.2min\n",
      "[CV] learning_rate=0.001, n_estimators=500 ...........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  4.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. learning_rate=0.01, n_estimators=1000, score=0.933252 - 1.2min\n",
      "[CV] learning_rate=0.001, n_estimators=500 ...........................\n",
      "[CV] .. learning_rate=0.01, n_estimators=1000, score=0.934543 - 1.2min\n",
      "[CV] learning_rate=0.001, n_estimators=500 ...........................\n",
      "[CV] .. learning_rate=0.001, n_estimators=500, score=0.928410 -  36.2s\n",
      "[CV] learning_rate=0.001, n_estimators=500 ...........................\n",
      "[CV] .. learning_rate=0.001, n_estimators=500, score=0.928408 -  35.9s\n",
      "[CV] learning_rate=0.001, n_estimators=500 ...........................\n",
      "[CV] .. learning_rate=0.001, n_estimators=500, score=0.928408 -  36.3s\n",
      "[CV] learning_rate=0.001, n_estimators=1000 ..........................\n",
      "[CV] .. learning_rate=0.01, n_estimators=1000, score=0.932961 - 1.2min\n",
      "[CV] learning_rate=0.001, n_estimators=1000 ..........................\n",
      "[CV] .. learning_rate=0.001, n_estimators=500, score=0.928408 -  38.2s\n",
      "[CV] learning_rate=0.001, n_estimators=1000 ..........................\n",
      "[CV] .. learning_rate=0.001, n_estimators=500, score=0.928408 -  38.2s\n",
      "[CV] learning_rate=0.001, n_estimators=1000 ..........................\n",
      "[CV] . learning_rate=0.001, n_estimators=1000, score=0.928410 - 1.2min\n",
      "[CV] learning_rate=0.001, n_estimators=1000 ..........................\n",
      "[CV] . learning_rate=0.001, n_estimators=1000, score=0.928408 - 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  27 out of  30 | elapsed:  6.4min remaining:   43.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . learning_rate=0.001, n_estimators=1000, score=0.928408 - 1.2min\n",
      "[CV] . learning_rate=0.001, n_estimators=1000, score=0.928408 - 1.2min\n",
      "[CV] . learning_rate=0.001, n_estimators=1000, score=0.928408 - 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=4, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=4,\n",
       "       param_grid={'n_estimators': [500, 1000], 'learning_rate': [0.05, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=10)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid search\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {'n_estimators':[500,1000],\n",
    "        'learning_rate': [0.05, 0.01, 0.001]}\n",
    "\n",
    "clf = GridSearchCV( xgb.XGBClassifier(max_depth=4), parameters, n_jobs=4, cv=5, verbose = 10)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission\n",
    "predictions = list(pred) \n",
    "predictions = zip(range(len(pred)), predictions)\n",
    "with open(\"result_pred.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow([\"ID\", \"category\"])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"result_pred.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data_set = list(reader)\n",
    "    data_set = [[element[0].split(\" \") for element in data_set],[element[1].split(\" \") for element in data_set]]  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# result_list = result_list.apply(lambda x:x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(data_set[1][i] for i in range(1,len(data_set[0])))\n",
    "result.index = np.arange(1, len(result) + 1)\n",
    "result.index.name = 'target id'\n",
    "result.columns = ['label']\n",
    "# features[\"link\"] = np.nan\n",
    "\n",
    "# result['target id'] = result['target id'].apply(lambda x:x[0])\n",
    "# result['label'] = result['label'].apply(lambda x:x[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result.iloc[0:10000].to_csv('result_pred_1.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98353\n"
     ]
    }
   ],
   "source": [
    "print(len(features2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TestData\n",
    "features_test = create_features(testdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "graph_testdata = create_graph(features_test)\n",
    "graph_testdata_2 = create_digraph(features_test)\n",
    "\n",
    "no_common_neighbors_testdata = common_neighbors(features_test, graph_testdata)\n",
    "features_test['No_common_neighbors'] = no_common_neighbors_testdata\n",
    "\n",
    "Jaccard_testdata = Jaccard_coef(features_test, graph_testdata)\n",
    "features_test['Jaccard_coef'] = Jaccard_testdata\n",
    "\n",
    "btw_diff_testdata = betweenness_diff(features_test, graph_testdata)\n",
    "features_test['Betweenness_diff'] = btw_diff_testdata\n",
    "\n",
    "\n",
    "diff3 = in_link_diff(features_test, graph_testdata_2)\n",
    "features_test['In_link_diff'] = diff3\n",
    "\n",
    "partition3 = community.best_partition(graph_testdata)\n",
    "same_cluster_testdata = is_same_cluster(partition3, features_test)\n",
    "features_test['Is_same_cluster'] = same_cluster_testdata\n",
    "\n",
    "# result_list.iloc[0:10000].to_csv('result.csv', sep=',')\n",
    "features_test.to_csv('graph_features_testing_data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "testing = pd.read_csv('graph_features_testing_data.csv')\n",
    "del testing['Unnamed: 0']\n",
    "\n",
    "X_test2 = testing.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test = clf.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test = gbm.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#submission\n",
    "predictions = list(pred_test) \n",
    "predictions = zip(range(len(pred_test)), predictions)\n",
    "with open(\"result_pred_2.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow([\"ID\", \"category\"])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"result_pred_2.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data_set = list(reader)\n",
    "    data_set = [[element[0].split(\" \") for element in data_set],[element[1].split(\" \") for element in data_set]]  \n",
    "    \n",
    "result = pd.DataFrame(data_set[1][i] for i in range(1,len(data_set[0])))\n",
    "result.index = np.arange(1, len(result) + 1)\n",
    "result.index.name = 'target id'\n",
    "result.columns = ['label']\n",
    "result.iloc[0:10000].to_csv('result_pred_2.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
